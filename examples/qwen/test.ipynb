{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dbb0680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxy/miniconda3/envs/diffu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 1. 只让内核看到 4 号和 5 号卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"      # 写在最前面！\n",
    "\n",
    "model_path = '/data/lxy/Qwen/qwen2.5-7b-base'\n",
    "\n",
    "# 2. 加载时交给 accelerate 自动拆\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",        # 关键！\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e603fdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:01<00:00,  3.11it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 87,293,952 || all params: 8,278,029,312 || trainable%: 1.0545\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 1. 只让内核看到 4 号和 5 号卡\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"      # 写在最前面！\n",
    "\n",
    "model_path = '/data/lxy/Qwen/qwen3-8b-base'\n",
    "\n",
    "# 2. 加载时交给 accelerate 自动拆\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",        # 关键！\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c18a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxy/miniconda3/envs/diffu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,099,467,264 || trainable%: 1.0357\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "from transformers import AutoModelForCausalLM,AutoModel,AutoModelForMaskedLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"      # 写在最前面！\n",
    "\n",
    "model_path='/data/lxy/diffusion/llada-8b'\n",
    "\n",
    "model=AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True, \n",
    "    device_map=\"auto\",        # 关键！\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce36016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from dataclasses import dataclass, field,asdict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = \"/data/lxy/diffusion/qwen2.5-7b-base\"\n",
    "    dtype: str = \"bfloat16\"\n",
    "    # --- fold PEFT args here ---\n",
    "    lora: bool = False\n",
    "    target_modules: str = \"all-linear\"\n",
    "    r: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.05\n",
    "    bias: str = \"none\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    dataset_args: str = None # overwrite this\n",
    "    num_proc: int = 8\n",
    "    disable_caching: bool = False\n",
    "    max_length: int = 1024\n",
    "    truncation: str = field(\n",
    "        default=\"right\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                'The truncation strategy to use (\"filter\" or \"right\"). '\n",
    "                '\"filter\" only keeps sequences that are shorter than max_length; '\n",
    "                '\"right\" only keeps the rightmost max_length tokens for each sequence.'\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    output_dir: str = None  # overwrite this\n",
    "    report_to: str = \"swanlab\"\n",
    "    run_name: str = \"test-1\"\n",
    "    overwrite_output_dir: bool = True\n",
    "    seed: int = 42\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    learning_rate: float = 2e-5\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    warmup_ratio: float = 0.1\n",
    "    bf16: bool = True\n",
    "    num_train_epochs: float = 6\n",
    "    logging_steps: float = 10\n",
    "    eval_on_start: bool = False\n",
    "    eval_strategy: str = \"steps\"\n",
    "    eval_steps: float = 0.25\n",
    "    save_steps: float = 0.25\n",
    "    save_only_model: bool = True\n",
    "    save_total_limit: int = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7583ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数： ModelArguments(model_name_or_path='/data/lxy/diffusion/qwen2.5-7b-base', dtype='bfloat16', lora=True, target_modules='all-linear', r=32, lora_alpha=64, lora_dropout=0.05, bias='none')\n",
      "数据参数： DataArguments(dataset_args='/data/lxy/diffusion/data/alpaca-zh-gpt[train:2000,test:200]', num_proc=8, disable_caching=False, max_length=1024, truncation='right')\n",
      "训练参数： TrainingArguments(\n",
      "_n_gpu=8,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=True,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=0.25,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=8,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_revision=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=no,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "liger_kernel_config=None,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/data/lxy/diffusion/output/qwen2.5-7b-alpaca-zh-gpt[train:2000,test:200]-epoch-200/runs/Jan13_14-57-42_T001,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=200,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/data/lxy/diffusion/output/qwen2.5-7b-alpaca-zh-gpt[train:2000,test:200]-epoch-200,\n",
      "overwrite_output_dir=True,\n",
      "parallelism_config=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "project=huggingface,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['swanlab'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=llada-qwen2.5-7b-alpaca-zh-epoch-200,\n",
      "save_on_each_node=False,\n",
      "save_only_model=True,\n",
      "save_safetensors=True,\n",
      "save_steps=0.25,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "trackio_space_id=trackio,\n",
      "use_cpu=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "模型参数字典： {'model_name_or_path': '/data/lxy/diffusion/qwen2.5-7b-base', 'dtype': 'bfloat16', 'lora': True, 'target_modules': 'all-linear', 'r': 32, 'lora_alpha': 64, 'lora_dropout': 0.05, 'bias': 'none'}\n",
      "数据参数字典： {'dataset_args': '/data/lxy/diffusion/data/alpaca-zh-gpt[train:2000,test:200]', 'num_proc': 8, 'disable_caching': False, 'max_length': 1024, 'truncation': 'right'}\n",
      "训练参数字典： {'output_dir': '/data/lxy/diffusion/output/qwen2.5-7b-alpaca-zh-gpt[train:2000,test:200]-epoch-200', 'overwrite_output_dir': True, 'do_train': False, 'do_eval': True, 'do_predict': False, 'eval_strategy': <IntervalStrategy.STEPS: 'steps'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 8, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 200, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.COSINE: 'cosine'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.1, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/data/lxy/diffusion/output/qwen2.5-7b-alpaca-zh-gpt[train:2000,test:200]-epoch-200/runs/Jan13_14-57-42_T001', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 10, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 0.25, 'save_total_limit': 2, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': True, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 0.25, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': 'llada-qwen2.5-7b-alpaca-zh-epoch-200', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'parallelism_config': None, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['swanlab'], 'project': 'huggingface', 'trackio_space_id': 'trackio', 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 8, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': 'no', 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = transformers.HfArgumentParser(\n",
    "    (ModelArguments, DataArguments, TrainingArguments)\n",
    ")\n",
    "\n",
    "config_yaml='./configs/qwen2.5-7b-alpaca.yaml'\n",
    "model_args, data_args, training_args = parser.parse_yaml_file(config_yaml)\n",
    "\n",
    "print(\"模型参数：\", model_args)\n",
    "print(\"数据参数：\", data_args)\n",
    "print(\"训练参数：\", training_args)\n",
    "\n",
    "model_d=asdict(model_args)\n",
    "data_d=asdict(data_args)\n",
    "training_d=asdict(training_args)\n",
    "print(\"模型参数字典：\", model_d)\n",
    "print(\"数据参数字典：\", data_d)\n",
    "print(\"训练参数字典：\", training_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348421ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_spec(spec: str):\n",
    "    \"\"\"\n",
    "    Parse a general 'name[a:b,c:d]' or 'a=b,c=d' style specification.\n",
    "\n",
    "    Supports:\n",
    "      - Bare name, e.g. \"foo/bar\"\n",
    "      - Optional bracket suffix with comma-separated entries:\n",
    "          key:value or key:int_value (underscores allowed)\n",
    "      - Optional \"key=value\" pairs outside the bracket.\n",
    "\n",
    "    Returns:\n",
    "      name: str or None\n",
    "      kv_dict: dict of key/value pairs (all combined)\n",
    "    \"\"\"\n",
    "\n",
    "    def _parse_kv_string(s: str) -> dict:\n",
    "        \"\"\"Parse comma-separated key=value pairs, e.g. 'a=1,b=2'.\"\"\"\n",
    "        return dict(part.split(\"=\", 1) for part in s.split(\",\") if \"=\" in part)\n",
    "\n",
    "    s = spec.strip()\n",
    "\n",
    "    # Extract bracket content if present\n",
    "    m = re.search(r\"\\[(.*?)\\]$\", s)\n",
    "    bracket_kvs = {}\n",
    "    numeric_kvs = {}\n",
    "    if m:\n",
    "        bracket = m.group(1).strip()\n",
    "        if bracket:\n",
    "            for part in bracket.split(\",\"):\n",
    "                part = part.strip()\n",
    "                if not part:\n",
    "                    continue\n",
    "                if \":\" not in part:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid entry '{part}' in '{spec}' (expected key:value).\"\n",
    "                    )\n",
    "                key, value = part.split(\":\", 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "\n",
    "                # Integers (with optional underscores)\n",
    "                if re.fullmatch(r\"\\d(?:_?\\d)*\", value):\n",
    "                    numeric_kvs[key] = int(value.replace(\"_\", \"\"))\n",
    "                else:\n",
    "                    bracket_kvs[key] = value\n",
    "\n",
    "        # Remove the bracket suffix from the working string\n",
    "        s = s[: m.start()].rstrip()\n",
    "\n",
    "    # Determine name (if any) and parse outer kvs (if any)\n",
    "    name = None\n",
    "    if \"=\" in s:\n",
    "        kv_dict = dict(_parse_kv_string(s))\n",
    "    else:\n",
    "        kv_dict = {}\n",
    "        if s:\n",
    "            name = s  # could represent a dataset, resource, or identifier\n",
    "\n",
    "    # Merge: bracket options and numeric keys last\n",
    "    kv_dict.update(bracket_kvs)\n",
    "    kv_dict.update(numeric_kvs)\n",
    "\n",
    "    return name, kv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f81ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/lxy/diffusion/data/alpaca-zh-gpt[train:2000,test:200]']\n",
      "/data/lxy/diffusion/data/alpaca-zh-gpt\n",
      "{'train': 2000, 'test': 200}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dataset_args='/data/lxy/diffusion/data/alpaca-zh-gpt[train:2000,test:200]'\n",
    "specs = [p.strip() for p in re.split(r\"[|+]\", dataset_args) if p.strip()]\n",
    "print(specs)\n",
    "\n",
    "\n",
    "dataset_name_or_path, kvs = parse_spec(specs[0])\n",
    "print(dataset_name_or_path)\n",
    "print(kvs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fcfda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'labels', 'prompt_len'],\n",
      "    num_rows: 48818\n",
      "})\n",
      "{'input_ids': [126080, 27, 91, 7351, 20679, 2983, 95591, 3840, 27, 91, 486, 20679, 2983, 95591, 198, 198, 5583, 30345, 7009, 12767, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'prompt_len': 43}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "orig_data_path='/data/lxy/diffusion/data/alpaca-zh-gpt'\n",
    "orig_dataset = load_from_disk(orig_data_path)\n",
    "print(orig_dataset)\n",
    "\n",
    "print(orig_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33644b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [126080, 27, 91, 7351, 20679, 2983, 95591, 3840, 27, 91, 486, 20679, 2983, 95591, 198, 198, 5583, 30345, 7009, 12767, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'prompt_len': 43}\n",
      "{'input_ids': [126080, 27, 91, 7351, 20679, 2983, 95591, 3840, 27, 91, 486, 20679, 2983, 95591, 198, 198, 2773, 7009, 65485, 6320, 2330, 4757, 18509, 19007, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198, 19007, 351, 629, 198, 4757, 629, 11781, 11058, 4682, 1504, 25052, 12703, 25900, 65485, 82879, 3119, 1974, 311, 198, 23795, 3348, 629, 46317, 2880, 115448, 74293, 71755, 1974, 7922, 15520, 42247, 311, 198, 198, 19007, 1101, 629, 198, 4757, 34066, 89101, 17207, 99781, 6838, 5993, 1468, 5838, 90022, 3119, 5289, 1362, 8394, 924, 19889, 311, 198, 23795, 3348, 629, 19997, 1066, 1984, 1165, 76934, 23634, 33029, 70886, 3594, 69990, 8269, 589, 6134, 11161, 5708, 2212, 531, 7008, 28737, 7914, 3412, 311, 198, 198, 19007, 1018, 629, 198, 4757, 629, 3119, 20264, 1131, 40441, 14600, 1504, 89101, 23764, 311, 198, 23795, 3348, 629, 2812, 33029, 49467, 8511, 40146, 52645, 10042, 12638, 1120, 3276, 11058, 20264, 1974, 50745, 1996, 5867, 4682, 44053, 18338, 22411, 32502, 108334, 11058, 6246, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19007, 351, 629, 198, 4757, 629, 11781, 11058, 4682, 1504, 25052, 12703, 25900, 65485, 82879, 3119, 1974, 311, 198, 23795, 3348, 629, 46317, 2880, 115448, 74293, 71755, 1974, 7922, 15520, 42247, 311, 198, 198, 19007, 1101, 629, 198, 4757, 34066, 89101, 17207, 99781, 6838, 5993, 1468, 5838, 90022, 3119, 5289, 1362, 8394, 924, 19889, 311, 198, 23795, 3348, 629, 19997, 1066, 1984, 1165, 76934, 23634, 33029, 70886, 3594, 69990, 8269, 589, 6134, 11161, 5708, 2212, 531, 7008, 28737, 7914, 3412, 311, 198, 198, 19007, 1018, 629, 198, 4757, 629, 3119, 20264, 1131, 40441, 14600, 1504, 89101, 23764, 311, 198, 23795, 3348, 629, 2812, 33029, 49467, 8511, 40146, 52645, 10042, 12638, 1120, 3276, 11058, 20264, 1974, 50745, 1996, 5867, 4682, 44053, 18338, 22411, 32502, 108334, 11058, 6246, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'prompt_len': 47}\n"
     ]
    }
   ],
   "source": [
    "train_data=orig_dataset.select(range(2000))\n",
    "test_data=orig_dataset.select(range(2000,2200))\n",
    "\n",
    "print(train_data[0])\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa6a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets=orig_dataset.select(range(kvs.get(\"train\",len(orig_dataset))))\n",
    "test_datasets=orig_dataset.select(range(kvs.get(\"train\",len(orig_dataset)),kvs.get(\"train\",len(orig_dataset))+kvs.get(\"test\",len(orig_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089440e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14ec3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [126080, 27, 91, 7351, 20679, 2983, 95591, 3840, 27, 91, 486, 20679, 2983, 95591, 198, 198, 5583, 30345, 7009, 12767, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 50219, 5583, 30345, 7009, 12767, 629, 198, 198, 16, 13, 112204, 4426, 3004, 51234, 1262, 14364, 19586, 5153, 5353, 42347, 389, 33687, 1193, 25327, 9428, 8712, 61409, 4659, 31874, 18956, 5938, 2366, 30057, 6631, 25490, 311, 198, 198, 17, 13, 220, 28836, 12936, 51234, 17813, 59260, 16692, 88873, 34672, 77601, 538, 17512, 15771, 34739, 22426, 9126, 18654, 851, 6373, 10187, 17512, 538, 8280, 8618, 2589, 5583, 30345, 72685, 311, 198, 198, 18, 13, 220, 20248, 22605, 311, 20248, 69347, 4659, 55041, 33897, 16953, 7877, 995, 5780, 220, 22, 12, 23, 220, 55336, 20248, 311, 11262, 20248, 30057, 22894, 6513, 24343, 4426, 8027, 2366, 4043, 4214, 20229, 73536, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'prompt_len': 43}\n",
      "{'input_ids': [126080, 27, 91, 7351, 20679, 2983, 95591, 3840, 27, 91, 486, 20679, 2983, 95591, 198, 198, 2773, 7009, 65485, 6320, 2330, 4757, 18509, 19007, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198, 19007, 351, 629, 198, 4757, 629, 11781, 11058, 4682, 1504, 25052, 12703, 25900, 65485, 82879, 3119, 1974, 311, 198, 23795, 3348, 629, 46317, 2880, 115448, 74293, 71755, 1974, 7922, 15520, 42247, 311, 198, 198, 19007, 1101, 629, 198, 4757, 34066, 89101, 17207, 99781, 6838, 5993, 1468, 5838, 90022, 3119, 5289, 1362, 8394, 924, 19889, 311, 198, 23795, 3348, 629, 19997, 1066, 1984, 1165, 76934, 23634, 33029, 70886, 3594, 69990, 8269, 589, 6134, 11161, 5708, 2212, 531, 7008, 28737, 7914, 3412, 311, 198, 198, 19007, 1018, 629, 198, 4757, 629, 3119, 20264, 1131, 40441, 14600, 1504, 89101, 23764, 311, 198, 23795, 3348, 629, 2812, 33029, 49467, 8511, 40146, 52645, 10042, 12638, 1120, 3276, 11058, 20264, 1974, 50745, 1996, 5867, 4682, 44053, 18338, 22411, 32502, 108334, 11058, 6246, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19007, 351, 629, 198, 4757, 629, 11781, 11058, 4682, 1504, 25052, 12703, 25900, 65485, 82879, 3119, 1974, 311, 198, 23795, 3348, 629, 46317, 2880, 115448, 74293, 71755, 1974, 7922, 15520, 42247, 311, 198, 198, 19007, 1101, 629, 198, 4757, 34066, 89101, 17207, 99781, 6838, 5993, 1468, 5838, 90022, 3119, 5289, 1362, 8394, 924, 19889, 311, 198, 23795, 3348, 629, 19997, 1066, 1984, 1165, 76934, 23634, 33029, 70886, 3594, 69990, 8269, 589, 6134, 11161, 5708, 2212, 531, 7008, 28737, 7914, 3412, 311, 198, 198, 19007, 1018, 629, 198, 4757, 629, 3119, 20264, 1131, 40441, 14600, 1504, 89101, 23764, 311, 198, 23795, 3348, 629, 2812, 33029, 49467, 8511, 40146, 52645, 10042, 12638, 1120, 3276, 11058, 20264, 1974, 50745, 1996, 5867, 4682, 44053, 18338, 22411, 32502, 108334, 11058, 6246, 311, 27, 91, 68, 335, 2983, 91, 3583, 91, 7351, 20679, 2983, 95591, 598, 10450, 27, 91, 486, 20679, 2983, 95591, 198, 198], 'prompt_len': 47}\n"
     ]
    }
   ],
   "source": [
    "print(train_datasets[0])\n",
    "print(test_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0cdd684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels', 'prompt_len'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels', 'prompt_len'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_from_disk\n",
    "results=DatasetDict({\"train\":train_datasets,\"test\":test_datasets})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70050e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
